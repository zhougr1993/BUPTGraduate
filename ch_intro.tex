%%
%% This is file `example/ch_intro.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% install/buptgraduatethesis.dtx  (with options: `ch-intro')
%% 
%% This file is a part of the example of BUPTGraduateThesis.
%% 

\chapter{绪论}

\section{研究背景及选题意义}
\subsection{研究背景及国内外研究现状}
随着时代的发展，我们积累了前所未有的大规模的文本数据。在这些文本数据中包含了大量有价值的信息。如何在海量的数据中寻找出文字间的语义关系已经成为了许多领域的重要问题。如搜索领域如果直接计算文档相似度那么会面临无法承受的计算量同时会受到语义漂移的影响。主题模型可以为每一篇文档分配一个主题，为处理海量的文档集合提供了一个更有前景和更能拓展的方法。

主题模型是一种对文本信息隐含主题进行建模的方法，目标是从文本信息中提取出隐含的主题来标识文本的语义信息。对比传统的信息检索中用到的文档相似度计算方法，主题模型所提取出的信息不仅仅是文档间词分布的相似，还提取出了更深一层更具泛华性的语义信息。基于这些主题，我们可以解决跨域文本分类\cite{Barathi}，理解文本聚类\cite{TopicClust}，文本推荐\cite{krestel2009latent}和其他相关文本数据应用的问题。

这些年来各个方向的研究人员们已经对从语料库进行的无监督学习进行了全面研究，潜在主题模型在现有方法中发挥了核心作用。主题模型根据能观测到的单词在文档中的分布从语料库中提取潜在的主题分布，由这些主题分布代表潜在语义空间中的文档。这种潜在语义空间的表达弥合了文档和单词之间的差距，从而实现了语料库的高效处理，如浏览，聚类和可视化。 PLSA\cite{PLSI}和LDA\cite{LDA}是两个众所周知的文档建模主题模型，将每个文档看做是不同主题（topic）的混合组成，同时每个文档的生成过程也是在这些主题中根据单个主题（topic）内的单词（word）分布来选择单词。
目前经典的主题模型PLSA，LDA已经被各个领域所广泛应用。不论是学术界还是工业界，主题模型都取得了不错的成果。但是大部分的研究工作都是集中在对PLSA和LDA这种生成式模型的扩展和优化上。这类生成模型可以通过对潜在分布的建模来解释观测到数据的分布，比如文档方面可以解释为什么有些部分是相似的。最关键是PLSA和LDA对于主题分布有一个比较漂亮的数学解释，其实PLSA和LDA本质上就是做了一个在概率空间上，对于文档（doc）和单词（word）共现矩阵的一个矩阵分解，这个矩阵分解的基就是主题分布。那么这些主题分布可以用于文档的分类，文档语义相似度的计算，以及单词（word）的主题分布本身也能作为一种词向量，词表示。可以用于自然语言处理各个方面的研究上。

在主题生成这个问题上，可观测到的数据是每一个单词和文档的共现关系（即一个单词（word）在文档（doc）中出现一次算作（word）和（doc）共现一次）。生成模型把整体的单词（word）和文档（doc）的共现矩阵看做是两个相对独立的多项分布p(w|t)和p(t|d) 的乘积，其中w代表单词，d代表文档，t代表主题。对于PLSA和LDA，他们的优化目标都是找到一个p(w|t)和p(t|d)的分布使得整体的似然度最大。但是这个优化问题整体是一个非凸的优化问题，因此会存在非常多的局部最优解。Blei在2010年的研究表明，现有的优化方法都存在一个问题，即对于同一份数据集，用同样的一种优化算法，以最大似然为目标分别运行多次，最终会获得不同的模型。

同时对于PLSA和LDA都有一个很大的问题，主题数$K$是需要预先人工设定好的，并且最终的模型的效果和主题数K密切相关。但是主题数$K$是很难通过先验知识得到的，对于主题数$K$有许多相关研究，但是并未提出有效的方法。目前一个主流的做法是，扩大主题数$K$，因为主题数K相当于拟合模型空间的秩，秩越大模型的表达能力就越强。但是Blei2010年的研究也表明一个更大的主题数$K$之下推测得到的模型效果并不能比一个合适的主题数$K$之下推测得到的模型效果好。

因此本文希望能探索出一种新的主题建模学习方式，解决主题数$K$在以往被当做一个超参数的问题，同时这个超参还对最终结果影响非常明显。
并且主题模型的应用很多时候被局限在了自然语言处理领域的一些子任务上，其实当把词和文档的意义平移到其他领域也能有效的运用主题模型，本文会尝试把主题模型平移到推荐领域。

同时在对语料进行无监督学习的整个方向中，衍生出了另一个方向，词嵌入式表达（word embedding）学习。词嵌入式表达\cite{word2vec,wordemb1,wordemb2,wordemb3}（word embedding）学习的思路是通过一些和最终任务不是直接关联的监督信息，如单词（word）的N-gram\cite{BRODER19971157}信息，来学习单词（word）或者文档（doc）在潜层语义空间中的表达。而这个思路从观测量--单词在文档集各个文档中的分布，最终推断的隐藏量--单词和文档的潜层语义空间表达，都是相通的。在自然语言处理领域有工作将两者结合起来，比如将主题模型的结果作为词嵌入式表达（word embedding）学习的先验知识来引导其训练。本文不会去具体探究在自然语言处理子任务上的优化，而是试图探索两者在商品推荐领域的一些应用和创新。

自硬件上面的发展取得质变后，通信和计算效率的提高为后续互联网的蓬勃发展提供广阔的空间。当下互联网的各种应用充斥在人们各种各样的生活中，我们的生活在每一天都面临着各种各样的选择，看什么电影，去哪家餐馆吃饭，住哪一家旅店。这些决策的候选集非常之大，给人们带来很大的困扰，人们要做出这些决策常常是非常痛苦的。如果有一个系统能帮助人们极大的缩小整个候选范围，这个系统无论是促进消费者更快捷的消费，还是在帮助用户找到自己更需要的商品上，都是非常有意义的。因此在当下推荐系统也是非常热门的一个研究领域，也有着非常多的研究成果。

诚然，主题模型和词嵌入式(word embedding)学习应当被归属于自然语言处理（NLP）领域，很多时候被大家应用在信息检索\cite{IR}，或是信息判别等各个领域。但是有一个被大家忽视的点在于，无论是信息检索领域还是推荐系统，要解决的问题都很类似，即信息筛选问题，从广袤的信息中抽取出真正被需要的信息，而主题模型和词嵌入（word embedding）技术的问题源头也是这样，都是为了更好的表达出每篇文档的语义。既然问题的出发点是同源的，其应用上就不应当彼此隔离。这些年来主题模型和词嵌入（word embedding）的技术在各个自然语言处理（NLP）子任务上都取得了长足的进展，然而在推荐系统领域的应用还几乎没有。因此将主题模型和词嵌入（word embedding）等自然语言处理领域的核心技术良好的应用到推荐系统领域，也是一个不错的创新思路。本文会仔细分析推荐系统于主体模型以及词嵌入（word embedding）的关系，并将这些技术应用到商品推荐系统上。
\subsection{课题主要来源}
本论文的课题研究主要基于对主题模型的研究创新提出了一种新的主体模型HLSM，同时与世界最大的电商平台--淘宝网合作，研究了主体模型和词嵌入技术在商品推荐领域的应用，所有的实验数据都是工业级的真实数据，验证效果的数据也是真实场景下的数据。
\section{主要研究内容}
本文研究的主要内容集中在主体模型领域，试图探索出一种新的根据语料库自适应的主题建模方法，能够提高整个模型对文档的表达效果，为主题模型提供一种新的思路。同时聚焦于主体模型和词嵌入式表达（word embedding）学习这些对文档集的无监督学习方法，它们都试图于获得语料集合的潜层语义空间表达，从而应用到其他的自然语言处理子任务上。本文会分析其和商品推荐问题的关联，探索这些方法在商品推荐领域的应用，并用实际的工业级的数据来客观实验，验证想法和最终效果。

具体的研究工作如下：

一、提出新的自适应主题建模方法

PLSA和LDA在解决主题生成的问题上，都需要人工设定一个主题数$K$，而且模型的效果和这个主题数$K$密切相关。而大多数情况下主题数$K$是无法通过先验知识得到的，只能通过人为的迭代尝试得到一个比较好的解。在许多对主题模型的研究中都提到了寻找主题数K的问题，然而目前并没有一个很好的解决方案。

事实上，PLSA和LDA都认为文档之间的不同主题中单词的分布$P(word|topic)$是相互独立的，因此主题数$K$其实可以看做是主题建模求得主题概率分布的空间的秩，即这个解空间的自由度，不巧的是各个研究者的实验都表明这个自由度对最终模型效果的影响非常之大。因此人工的选择是非常不合理的方案。
如果我们将问题抽象到另一个角度，把所有词连接成一个网络，主题生成的过程其实类似于复杂网络中社区发现的问题。在物理学的研究领域，复杂网络社区分析方面有很多有效的研究可以自动发现社区个数与社区。因此希望借鉴社区发现的思想来自动发现主题个数与主题。同时之前的主题模型在主题粒度上并没有做特殊的优化，有一些关于分层主题的研究但是有效的成果比较少，主题模型应当对高层抽象的主题和底层具体的主题有不同的处理。本文综合这些方面的考虑，提出了一个新的自适应分层主题建模的方法，可根据语料库中能观测到的词语分布信息，自适应的发现主题个数，并且在具体层级的主题和抽象层级的主题上做出区分，以及抽象主题包含具体主题的关系。

二、探索主题模型和词嵌入式表达（word embedding）等潜层语义方法在商品推荐领域的应用
深入的分析整个主题模型作用的过程和原理也是一个重要的过程，因为这个本质了解清楚之后可以将这些理论泛化到其他的问题，而不是仅仅局限于语言信息处理领域。同时词嵌入式表达（word embedding）和主题模型的本质观测空间是相似的，都是这些被处理的语料库中所有单词在文档中的分布情况，细小区别是有的词嵌入式表达（word embedding）方法如word2vec\cite{word2vec}的观测粒度更细，有窗口的概念。同时其希望推断的隐藏变量也是相似的，主题模型是概率空间约束下的文档和单词的表达，而词嵌入式表达学习（word embedding）是一个对单词的向量空间表达，同时未明确约束其物理意义。

而商品推荐领域中研究的一个主要问题也是用户（user）和商品（item）的一个关系。如果我们能寻找到能包含这种关系信息的其各自向量空间表达。这对于商品推荐系统的最终推荐效果和其本身在不同场景解决问题迭代的效率都会有巨大的贡献。

目前的生成式的主题模型，其实是做了一个概率空间上，对文档（doc）和单词（word）共现矩阵的分解，分解矩阵的基是主题分布。而这个意义和推荐常用的协同过滤矩阵分解是相似的。而word2vec也是类似，对文档（doc）和单词（word）的PMI矩阵的分解。其实把用户（User）对商品（Item）的行为链映射为文档（doc），商品（Item）映射为词（word），就可以简单的把商品推荐和主题模型结合起来。

本文将传统主题建模的思路和词嵌入式表达（word embedding）的思路抽象到了商品推荐领域，值得注意的是本文的目的不仅仅是将主体模型和词嵌入（word embedding）技术做一个应用领域的迁移，而是会仔细的分析推荐系统和这两个技术的关系，以及为什么这些技术会在推荐系统有效。后文中会仔细介绍如何产生这个想法以及这个思路要解决的问题，并用工业级的数据做了客观的实验，验证了这方面的想法，以及其具体实验效果，并且最终证明了其有效性，最后在真实的商品推荐系统上上线了这一模型。

研究生期间，本人在“主题建模”课题上的主要研究成果包括：
  \begin{itemize}
  \item 以学生一作身份发表EI论文一篇。
  \end{itemize} 
\section{论文的结构安排}
全文内容安排如下：


%% 本章参考文献
%\ifx\usechapbib\empty
%\nocite{BSTcontrol}
%\setcounter{NAT@ctr}{0}
%\bibliographystyle{buptgraduatethesis}
%\bibliography{bare_thesis}
%\fi
